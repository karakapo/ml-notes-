# ðŸ§  Support Vector Machines (SVM) â€” Derinlemesine Ã–zet

Support Vector Machines (SVM), gÃ¼Ã§lÃ¼ bir denetimli Ã¶ÄŸrenme algoritmasÄ±dÄ±r. En Ã§ok **sÄ±nÄ±flandÄ±rma** problemlerinde kullanÄ±lÄ±r, fakat **regresyon** iÃ§in de uyarlanabilir (SVR).

---

## ðŸ” SVM Ne Yapar?

SVM'in temel hedefi:
> **Veri noktalarÄ±nÄ± ayÄ±ran doÄŸrusal bir karar sÄ±nÄ±rÄ± (hiper dÃ¼zlem) bulmak.**

AmaÃ§:
- EÄŸitim verisini **iki sÄ±nÄ±fa** ayÄ±ran **doÄŸru(lar)** bulmak.
- Bu doÄŸrular (veya hiper dÃ¼zlemler) arasÄ±ndan, sÄ±nÄ±flarÄ± ayÄ±ran en iyi olanÄ± seÃ§mek:  
  âž¤ Bu da **sÄ±nÄ±flara en yakÄ±n veri noktalarÄ±na olan mesafeyi (marjin) maksimize eden** doÄŸrudur.

---

## ðŸ§± Temel BileÅŸenler

### âœ… **Support Vector Nedir?**

- SÄ±nÄ±flandÄ±rma sÄ±nÄ±rÄ±na en yakÄ±n olan **veri noktalarÄ±dÄ±r.**
- Bu noktalar, karar sÄ±nÄ±rÄ±nÄ±n tam yerini belirler.
- **Modelin yalnÄ±zca bu noktalara gÃ¶re belirlenmesi**, SVMâ€™in "sparse" (seyrek) doÄŸasÄ±nÄ± oluÅŸturur.

### âš–ï¸ **C Parametresi ve Trade-off**

- **C**, **marjin geniÅŸliÄŸi ile sÄ±nÄ±flandÄ±rma hatasÄ± arasÄ±ndaki dengeyi** kontrol eder.
- KÃ¼Ã§Ã¼k C â†’ Daha bÃ¼yÃ¼k marjin ama bazÄ± yanlÄ±ÅŸ sÄ±nÄ±flandÄ±rmalar kabul edilir.
- BÃ¼yÃ¼k C â†’ HatalarÄ± tolere etmez, ama marjin kÃ¼Ã§Ã¼lÃ¼r ve **overfitting riski artar.**

---

## ðŸš€ SVMâ€™in GÃ¼cÃ¼ Nereden Geliyor?

### ðŸŽ¯ **Linearly Separable (DoÄŸrusal AyrÄ±labilir) Veriler Ä°Ã§in:**
- SVM maksimum marjinli **hiper dÃ¼zlem** bulur â†’ MÃ¼kemmel Ã§alÄ±ÅŸÄ±r.

### ðŸ”„ **Linearly Non-Separable Veriler Ä°Ã§in:**
- Veriyi baÅŸka bir uzaya **projeksiyonla taÅŸÄ±rÄ±z.**
- Bu yeni uzayda veri **doÄŸrusal ayrÄ±labilir hale gelir.**
- Bu sayede orada yine bir hiper dÃ¼zlem bulunabilir.

---

## ðŸ“ˆ Cover's Theorem

> "High-dimensional spaces tend to make data more separable."

- Yani, **veri boyutu arttÄ±kÃ§a doÄŸrusal ayrÄ±labilir hale gelme olasÄ±lÄ±ÄŸÄ± artar.**
- Bu SVMâ€™in projeksiyon stratejisine temel saÄŸlar:  
  Veri orijinal uzayda ayrÄ±lamÄ±yorsa, daha yÃ¼ksek boyutlara taÅŸÄ± â†’ AyÄ±r!

---

## ðŸ§  Kernel Trick â€” SVMâ€™in Gizli GÃ¼cÃ¼

SVMâ€™in matematiksel yapÄ±sÄ±nda, **veri noktalarÄ±nÄ±n kendisine deÄŸil, sadece aralarÄ±ndaki iÃ§ Ã§arpÄ±mlara** (dot product) ihtiyaÃ§ vardÄ±r.

### ðŸ§© Problem:
- Veriyi yÃ¼ksek (hatta sonsuz) boyutlara taÅŸÄ±mak **hesaplama olarak pahalÄ±dÄ±r.**

### âœ¨ Ã‡Ã¶zÃ¼m: Kernel Trick

> **Kernel fonksiyonu**, yÃ¼ksek boyutlu uzayda yapÄ±lacak iÃ§ Ã§arpÄ±mÄ±, doÄŸrudan orijinal uzaydaki iki nokta Ã¼zerinden hesaplar.

---

## ðŸ”¬ Kernel Nedir?

- Ä°ki veri noktasÄ±nÄ± alÄ±r, sanki yÃ¼ksek boyutlu bir uzaya projekte edilmiÅŸler gibi **iÃ§ Ã§arpÄ±m sonucunu** verir.
- BÃ¶ylece yÃ¼ksek boyutlara Ã§Ä±kmaya **gerek kalmaz**, ama etkisi alÄ±nÄ±r.

### ðŸŽ“ Teknik TanÄ±m:

Bir kernel fonksiyonu \( K(x, y) \),  
yÃ¼ksek boyutlu uzayda:  
\[
K(x, y) = \phi(x) \cdot \phi(y)
\]  
Ama biz doÄŸrudan:
\[
K(x, y) = \text{some function of } x \text{ and } y
\]
olarak hesaplarÄ±z.

---

## ðŸ“Š Neden Kernel KullanÄ±lÄ±r?

Ã–rnek:

Projeksiyon yaparsak:
- Her nokta iÃ§in 4 iÅŸlem â†’ 2 nokta = 8 iÅŸlem
- Dot product = 3 iÅŸlem â†’ toplam: **13 iÅŸlem**

Kernel ile:
- 2 Ã§arpma + 1 toplama + kare alma = **4 iÅŸlem**

Bu fark, yÃ¼ksek boyutlu veride **binlerce kat hÄ±z farkÄ±na** dÃ¶nÃ¼ÅŸÃ¼r.

---

## ðŸ§ª SÄ±k KullanÄ±lan Kernel FonksiyonlarÄ±

### 1. **Linear Kernel**  
\[
K(x, y) = x \cdot y
\]  
Orijinal uzaydaki iÃ§ Ã§arpÄ±m â€” hiÃ§bir projeksiyon yok.

---

### 2. **Polynomial Kernel**  
\[
K(x, y) = (x \cdot y + c)^d
\]  
- \(c\): Bias terimi  
- \(d\): Polinom derecesi  
- Ã–rnek: \(c = 0\), \(d = 2\) â†’ ikinci dereceden projeksiyon

---

### 3. **RBF (Radial Basis Function / Gaussian Kernel)**  
\[
K(x, y) = \exp(-\gamma \|x - y\|^2)
\]  
- Sonsuz boyutlu projeksiyon etkisi verir  
- Uzak noktalarÄ± sÄ±fÄ±ra yakÄ±nlaÅŸtÄ±rÄ±r, yakÄ±n noktalarÄ± 1â€™e yaklaÅŸtÄ±rÄ±r

---

### 4. **Sigmoid Kernel**  
\[
K(x, y) = \tanh(\alpha x \cdot y + c)
\]  
- Eski neural net aktivasyonlarÄ±na benzer  
- Ã‡oÄŸu durumda RBFâ€™den daha az etkilidir.

---

## ðŸŽ¯ Kernel SeÃ§iminde Pratik Notlar

- Her kernel her veri seti iÃ§in **iyi Ã§alÄ±ÅŸmaz.**
- BaÅŸlangÄ±Ã§ta:
  - Linear ve RBF kernel ile test yap
  - Sonra polynomial kernel ile varyasyonlar dene
- Grid Search + Cross Validation ile **en uygun kernel ve parametreler** bulunur.

---

## ðŸ”š Ã–zet

| BileÅŸen | AÃ§Ä±klama |
|--------|----------|
| SVM'in gÃ¶revi | Maksimum marjinli ayÄ±rÄ±cÄ± hiper dÃ¼zlemi bulmak |
| Support Vector | Karar sÄ±nÄ±rÄ±na en yakÄ±n noktalar |
| C parametresi | Marjin ve hata arasÄ±ndaki dengeyi ayarlar |
| Kernel | Projeksiyon yapmadan yÃ¼ksek boyutlu iÃ§ Ã§arpÄ±mÄ± saÄŸlar |
| RBF | Sonsuz boyut etkisi veren kernel |
| Cover's Theorem | Boyut arttÄ±kÃ§a ayrÄ±labilirlik artar |

---

> "With the right kernel, we donâ€™t need to know how the data is projected â€” only what the inner product would have been."  

---  
**ðŸ§  Kaynaklar:**  
- Vapnik, *Statistical Learning Theory*  
- scikit-learn SVM Documentation  
- CS229 - Stanford ML Lectures

